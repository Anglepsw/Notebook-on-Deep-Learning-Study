{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week5 笔记\n",
    "本周课程的内容分成3部分，包括涉及Bias/Variance、几种常见的正则化操作和一些实现DNN时必要的方法。\n",
    "\n",
    "## Setting up your Machine Learning Application\n",
    "这部分内容包括\n",
    "- Train/Dev/Test sets\n",
    "- Bias/Variance\n",
    "- Basic Recipe for Machine Learning\n",
    "\n",
    "**视频的一开始，NG不断地强调了一点：应用ML是一个不断迭代的过程！不断轮回！**\n",
    "\n",
    "接下里整理知识点\n",
    "##### 数据集的划分\n",
    "以往ML中使用的数据集都比较小，划分时候经常按60% 20% 20%或者70% 15% 15%来划分训练集、验证集和测试集。但DL中由于数据集往往都比较大，所以不这么划分，留给验证集和测试集的比例都很小，大约是98% 1% 1%来划分。比如1,000,000的数据集，留10,000做验证，留10,000做测试即可。\n",
    "##### 数据集的分布\n",
    "从概率上解释，我们所做的最小化代价函数本质就是在求取给定$x$下$y$的条件概率分布（一般不求全概率分布）。在选取模型时候，我们通过验证集进行模型选择，通过测试集测试模型性能，所以要求验证集和测试集是同分布的。否则根据验证集选出来的模型在测试集上的表现可能会很差。\n",
    "\n",
    "## Regularizing your neural network\n",
    "\n",
    "\n",
    "## Setting up your optimization problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
