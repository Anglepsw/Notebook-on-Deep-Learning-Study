{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week7 笔记\n",
    "本周的内容包括\n",
    "- 超参数的调节\n",
    "- Batch Normalization\n",
    "- 利用softmax函数实现多分类\n",
    "- DL的常见程序框架\n",
    "\n",
    "&emsp;&emsp;目前常见的DL程序框架无非就是TensorFlow之类的，NG重点介绍了Tensor FLow。这种框架一般的学习方法是了解该框架的基本结构和语法后，然后对照官方的说明文档直接边查变用，上手最快，这里就不做总结了。两个传送门：\n",
    "[TensorFlow中文社区](http://www.tensorfly.cn/  \"TensorFlow中文社区\")\n",
    "[TensorFlow官网](https://www.tensorflow.org/ \"TensorFlow官网\")（请根据需要科学上网，手动滑稽）\n",
    "\n",
    "接下来开始总结\n",
    "\n",
    "##### 超参数调节\n",
    "当前的超参数按照重要程度/优先程度排列，包括：\n",
    "1. 学习率$\\alpha$\n",
    "2. 动量梯度下降中系数$\\beta$，或Adam算法中系数$\\beta_{1}$、$\\beta_{2}$以及$\\epsilon$\n",
    "3. Mini-Batch size\n",
    "4. 隐含层单元数\n",
    "5. 网络层数和学习率下降速率\n",
    "\n",
    "&emsp;&emsp;以上意思是说，最重要的就是$\\alpha$，影响最大，需要仔细调。然后2和3都是可选项，4和5很少进行调整或是最后才调整或者是真的没招了才调整。\n",
    "\n",
    "&emsp;&emsp;对于超参数较少的情况，可以采用**画网格**的方法进行试验找出合适的。超参数个数非常多的时候，应当采用**均匀随机采样**的方法，在大范围里确定一个相对小的范围，然后再次**均匀随机采样**以获在训练集或是验证集表现好的超参数。\n",
    "\n",
    "&emsp;&emsp;在搜索的过程中，要注意所搜索的超参数定义域的分布以及超参数在某一区域是否对结果非常敏感。\n",
    "    \n",
    "    例如超参数$\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
