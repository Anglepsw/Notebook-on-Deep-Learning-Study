{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week5 笔记\n",
    "本周课程的内容分成3部分，包括涉及Bias/Variance、几种常见的正则化操作和一些实现DNN时必要的方法。\n",
    "- Setting up your Machine Learning Application\n",
    "- Regularizing your neural network\n",
    "- Setting up your optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setting up your Machine Learning Application\n",
    "这部分内容包括\n",
    "- Train/Dev/Test sets\n",
    "- Bias/Variance\n",
    "- Basic Recipe for Machine Learning\n",
    "\n",
    "**视频的一开始，NG不断地强调了一点：应用ML是一个不断迭代的过程！不断轮回！**\n",
    "\n",
    "接下里整理知识点\n",
    "##### 数据集的划分\n",
    "以往ML中使用的数据集都比较小，划分时候经常按60% 20% 20%或者70% 15% 15%来划分训练集、验证集和测试集。但DL中由于数据集往往都比较大，所以不这么划分，留给验证集和测试集的比例都很小，大约是98% 1% 1%来划分。比如1,000,000的数据集，留10,000做验证，留10,000做测试即可。  \n",
    "此外，一些人在划分数据集时会只画出训练集和验证集，即把验证集当作测试集用，这会导致过拟合的情况发生（**思考：为什么会过拟合？**）\n",
    "##### 数据集的分布\n",
    "从概率上解释，我们所做的最小化代价函数本质就是在求取给定$x$下$y$的条件概率分布（一般不求全概率分布）。在选取模型时候，我们通过验证集进行模型选择，通过测试集测试模型性能，所以要求验证集和测试集是同分布的。否则根据验证集选出来的模型在测试集上的表现可能会很差。\n",
    "##### Bias和Variance\n",
    "首先，明确Bias和Variance的意义。简单来理解，Bias就是欠拟合，而Variance则是过拟合。在ML中经常会强调Bias和Variance的平衡（Trade-off），因为在ML中没有工具能够做到只降低Bias或Variance中的一个而不影响另一个。而在DL中，能够实现只降低Bias而不增加Variance或是降低Variance而不增加Bias。因此，在DL中就不再提两者的平衡了。\n",
    "##### 根据Bias和Variance选择后续方法\n",
    "首先根据训练集和验证集确定Bias和Variance，判断是High Bias还是High Variance：\n",
    "1. High Bias情况：\n",
    "    - 增大网络规模\n",
    "    - 增加训练时间/迭代次数\n",
    "    - 更新网络结构\n",
    "2. High Variance情况：\n",
    "    - 收集更多数据\n",
    "    - 采用正则化策略\n",
    "    - 更新网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing your neural network\n",
    "这部分内容包括\n",
    "- Regularization\n",
    "- Why regularization reduces overfitting\n",
    "- Dropout regularization\n",
    "- Understanding dropout\n",
    "- Other regularization methods\n",
    "\n",
    "**由于网络的加深，使得其模型容量增加，更容易产生过拟合的现象。因此，需要采用一些方法避免过拟合。这里重点学会正则化因子和drop out两种方法。当然，如果数据集非常非常大，过拟合的现象会大大降低。**(所以，请问数据集多大才算大？NG在视频里经常提到的数据集大小是1,000,000、5,000,000)**正则化除了降低过拟合的作用外，还能帮助降低误差。**\n",
    "\n",
    "接下里开始内容总结\n",
    "\n",
    "##### 正则化因子\n",
    "通常网络学习时候的代价函数是\n",
    "$$J(w,b)=\\frac{1}{m}\\sum_{i=1}^m L(\\hat{y}^{(i)},\\hat{y})$$\n",
    "引入正则化因子和正则化项，代价函数变为\n",
    "$$J(w,b)=\\frac{1}{m}\\sum_{i=1}^m L(\\hat{y}^{(i)},\\hat{y})+\\frac{\\lambda}{2m}\\lVert{w}\\rVert_{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your optimization problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
